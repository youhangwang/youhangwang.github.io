---
title: Open Cluster Management Overview
tags: OCM Federation
--- 

Open Cluster Management (OCM) 是一个功能强大的、模块化的、可扩展的 Kubernetes 多集群编排平台。它吸取了之前 Kubernetes Federation系统的失败教训，OCM跳出了Kubefed v2的集中式，命令式的架构，采用类似 Kubernetes `hub-kubelet`的`hub-agent`架构。在 OCM 中，多集群控制平面被直观地建模为`Hub`，由`Hub`管理的每个集群都是`Klusterlet`。

- `Hub Cluster`：表示运行OCM多集群控制平面的集群。一般来说，Hub Cluster是一个轻量级的 Kubernetes 集群，只托管一些基本的控制器和服务。
- `Klusterlet`：表示由hub cluster管理的集群，也可以称为`managed cluster`或`spoke cluster`。`klusterlet`应该主动从中心集群中拉取最新的`prescriptions`，并不断尝试将集群调整到预期状态。

<!--more-->

## Hub-spoke

受益于“hub-spoke”架构的优点，大部分多集群操作被抽象的解耦为：
1. 计算/决策
2. 执行

而针对目标集群的实际操作将是完全由托管集群执行。`Hub Cluster`不会直接`Managed Cluster`发送请求，它只会为每个集群持久地保存prescriptions，`klusterlet`将主动从`Hub Cluster`中提取prescriptions并执行。由于`Hub Cluster`不需要处理来自托管集群的海量事件，也不需要忙于一直向集群发送请求，`Hub Cluster`的负担将大大减轻。想象一下，如果在`Kubernetes`中没有`kubelet`， 并且其控制平面直接操作容器守护进程，集中式的Controller想要管理 5k+ 个节点的集群将会非常困难。同样，将执行部分拆分并由独立的Agent执行，这也正是 OCM 所用来突破可扩展性瓶颈的方式。所以`Hub Cluster`可以管理上至数千个集群。

每个`klusterlet`都将独立自主地工作，因此它们对`Hub Cluster`的可用性具有弱依赖性。如果`Hub Cluster`出现故障（例如在维护或处于网络分区期间），在托管集群中工作的`klusterlet`或其他 OCM Agent应该继续管理托管集群，直到重新连接`Hub Cluster`。此外，如果`Hub Cluster`和托管集群由不同的管理员所管理，由于`klusterlet`作为`pod`运行在托管集群中，托管集群的管理员可以容易地监管从`Hub Cluster`控制平面获取到的处方。如果发生任何意外，`klusterlet`管理员可以快速切断与`Hub Cluster`的连接，而无需关闭整个多集群控制平面。

![ocm-arch](../../../assets/images/posts/ocm-arch.svg)

`Hub-spoke`架构最大限度地减少了将新集群注册到`Hub Cluster`中的网络要求。任何能够连接到`Hub Cluster`端点的集群都可以被管理。这是因为prescriptions是从`Hub Cluster`中拉取地而不是`Hub Cluster`主动Push。除此之外，OCM 还提供了一个名为`cluster-proxy`的插件，它基于`konnectivity`自动地管理一个反向代理隧道，可以主动地访问托管集群。


## Concepts
### Cluster registering: “double opt-in handshaking”

通常`Hub Cluster`和托管集群可以由不同的管理员维护。为防止未经批准的请求，OCM明确分离了角色，使集群注册需要双方的批准。在终止注册方面，Hub Cluster 管理员可以通过拒绝轮换Hub Cluster证书的来踢出已注册的集群，另一方面，从托管集群的admin角度来看，他可以粗暴地删除Agent或撤销已授予Agent的 RBAC 权限。`Hub Cluster`将自动为新注册的集群配置环境，并在踢出托管集群时清理。
![double-optin-registration](../../../assets/images/posts/double-optin-registration.svg)

### Cluster namespace

Kubernetes 对于不同Namespace中的资源具有原生的多租户隔离，因此在 OCM 中，对于每个托管集群，我们将为托管集群配置一个专用的命名空间并授予足够的 RBAC 权限，以便`klusterlet`可以保留一些数据到`Hub Cluster`中。这个专用命名空间是`Cluster namespace`，它主要用于保存来自`Hub Cluster`的prescriptions。例如我们可以在集群命名空间中创建`ManifestWork`，以便将一些资源部署到相应的集群。同时，集群命名空间也可用于保存从`klusterlet`上传的统计信息，例如插件的健康状态等等。

### Addons

Addon 是基于OCM可扩展性的, 可选的，可插拔的，定制化组件。它可以是`Hub Cluster`中的控制器，或者只是托管集群中的Agent，或者是一起协作的两者。Addon需要实现`ClusterManagementAddon`或`ManagedClusterAddon` [API](https://open-cluster-management.io/concepts/addon/)。

## ManagedCluster

`ManagedCluster`是`Hub Cluster`中的Cluster-Scoped API，它表示 OCM 中已注册或等待接受的 Kubernetes 集群。在托管集群中工作的 klusterlet agent会主动地维护/刷新`Hub Cluster`上相应的`ManagedCluster`资源状态。另一方面，从`Hub Cluster`中删除`ManagedCluster`表示该集群被从`Hub Cluster`中踢出。下面会介绍集群注册生命周期是如何工作的：

### Registration and Accepance
#### Bootstrapping registration

首先，集群注册过程应该由Registration Agent启动，它需要一个bootstrap kubeconfig，例如：
```
apiVersion: v1
kind: Secret
metadata:
  name: bootstrap-hub-kubeconfig
  namespace: open-cluster-management-agent
type: Opaque
data:
  kubeconfig: <base64-encoded kubeconfig>
```
bootstrap kubeconfig 中的subject所需的最小 RBAC 权限是：
- CertficateSigningRequest: “get”, “list”, “watch”, “create”, “update”.
- ManagedCluster: “get”, “list”, “create”, “update”.

理想情况下，bootstrap kubeconfig 应该在`Hub Cluster`接受后不久就会被删除，这样它就不会被其他客户端滥用。但更简单的方式是通过利用 OCM 的命令行工具`clusteradm`来管理整个注册过程。

#### Approving registration

在 OCM 中注册一个新集群时，registration agent 将首先在`Hub Cluster`中创建一个未接受的`ManagedCluster`以及一个临时`CertficateSigningRequest (CSR)`资源。如果满足以下要求，集群将被`Hub Cluster`接受：
- CSR被任意的Provider设置批准并使用合法的X.509证书填充`.status.certificate`字段。
- 将`ManagedCluster`资源中Spec的`.spec.hubAcceptsClient`字段设置为`true`来批准集群的加入。

上面的批准过程可以通过一行指令来完成：
```
clusteradm accept --clusters <cluster name>
```

批准后，registration agent将从Status中获取签名证书并将其保存为名为`hub-kubeconfig-secret`的本地`Secret`（默认在`open-cluster-management-agent`命名空间中），该密钥将被挂载到另一个`klusterlet`的基本组件中，例如work agent。总之，如果可以在托管集群中成功找到`hub-kubeconfig-secret`，则集群注册已设置完毕！

总体而言，OCM 中的注册过程称为双重选择机制，这意味着成功的集群注册需要`Hub Cluster`和托管集群双方的批准和承诺。当`Hub Cluster`和托管集群由不同的管理员或团队操作时，这将特别有用。在 OCM 中，我们假设集群在一开始是相互不信任的，然后在具有控制权限的情况下优雅地建立它们之间的连接。

上面提到的功能都是由 OCM 的[registration](https://github.com/open-cluster-management-io/registration)子项目管理的，它是 OCM 世界中的`根依赖`。它包括托管集群中的agent以注册到`Hub Cluster`和`Hub Cluster`中的控制器以与agent协调。

### Cluster Heartbeat and Status
默认情况下，注册将每隔一分钟向`Hub Cluster`报告和刷新其健康状态，并且可以通过在`ManagedCluster`上设置`.spec.leaseDurationSeconds`以覆盖该间隔。

除此之外，`ManagedCluster`的状态也会反映一些常用的信息，例如：
```
  status:
    version:
      kubernetes: v1.20.11
    allocatable:
      cpu: 11700m
      ephemeral-storage: "342068531454"
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 17474228Ki
      pods: "192"
    capacity:
      cpu: "12"
      ephemeral-storage: 371168112Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 23777972Ki
      pods: "192"
    conditions: ...
```
### Cluster Taints and Tolerations
为了过滤不健康/未报告的集群以防止工作负载被放置在不健康或无法访问的集群中，OCM引入了类似Kubernetes中的 taint/toleration 概念。它还允许用户添加自定义污点以从`Placement`中取消选择某个集群。当用户想要将集群设置为维护模式并从该集群中驱逐工作负载时，这很有用。

在 OCM 中，Taints 和 Tolerations 协同工作，让用户可以更灵活地控制如何选择托管集群。

#### Taints of ManagedClusters
Taints是`ManagedCluster`的属性，它们允许`Placement`将一组 ManagedCluster排除。Taints包括以下字段：

- Key（Required: 应用于集群的污点键。例如bar或foo.example.com/bar。
- Value(Optional): 污点键对应的污点值。
- Effect(Required): 污点对不能容忍污点的Placement的影响。有效效果是
  - NoSelect: 除非Placement具有这种污点的Tolerations，否则不允许Placement选择该集群。如果集群已被Placement选择，则将从placement decision中删除该集群。
  - PreferNoSelect。这意味着调度程序尝试不选择集群，而不是完全禁止Placements选择集群。
  - NoSelectIfNew。不允许Placement选择集群，除非： 1）Placement具有容忍污点； 2）placement decision中已经有了集群；
- TimeAdded(Required): 添加污点的时间。它是自动设置的，用户不应设置/更新其值。

内置污点会反映 ManagedClusters 的状态。目前有两个内置污点，他们会根据集群的条件自动添加到 ManagedClusters：
- `cluster.open-cluster-management.io/unavailable`
- `cluster.open-cluster-management.io/unreachable`

#### Tolerations of Placements
Tolerations 应用于 Placements，允许 Placements 选择具有匹配污点的 ManagedCluster。

### Cluster Removal
已经注册的集群可以选择从`Hub Cluster`或`Managment Cluster`切断连接。这有助于解决 OCM 环境中的问题，例如：

- 紧急情况下, `Hub Cluster`过载
- 当托管集群打算与 OCM 分离时
- 发现`Hub Cluster`向托管集群发送错误订单时
- 当托管集群向`Hub Cluster`发送垃圾请求时

#### Unregister from hub cluster

取消注册集群的推荐方法是将`.spec.hubAcceptsClient`设置回`false`，这将触发`Hub Cluster`控制平面从管理清单中去除该托管集群。同时，从`Hub Cluster`控制平面中踢出托管集群的一种永久方法是删除其`ManagedCluster`资源。
```
$ kubectl delete managedcluster <cluster name>
```

这也会在后台立即撤销之前授予托管集群的 RBAC 权限。如果我们希望将踢出集群推迟到下次`klusterlet`更新其证书时，可以从集群的角色资源中删除以下 RBAC 规则：
```
# ClusterRole: open-cluster-management:managedcluster:<cluster name>
# Removing the following RBAC rule to stop the certificate rotation.
- apiGroups:
    - register.open-cluster-management.io
  resources:
    - managedclusters/clientcertificates
  verbs:
    - renew
```
#### Unregister from the managed cluster

托管集群的管理员可以通过将 OCM `klusterlet` 的实例数量减少到 0 来禁用`Hub Cluster`的prescriptions。或者从托管集群中永久删除`klusterlet`组件。

### Certificate Rotation
托管集群中的代理用于与`Hub Cluster`控制平面对话的证书将定期轮换。下图显示了自动证书轮换的工作原理。
![registration-process]((../../../assets/images/posts/registration-process.svg)

## ManifestWork

`ManifestWork`用于定义`Hub Cluster`上的一组将要应用于托管集群的Kubernetes资源。在 OCM 项目中，必须在集群命名空间中创建`ManifestWork`资源。在托管集群中工作的work agent会监控`Hub Cluster`上集群命名空间中的 ManifestWork 资源。

以下示例显示了部署一个Deployment到托管集群的`ManifestWork`示例：
```
apiVersion: work.open-cluster-management.io/v1
kind: ManifestWork
metadata:
  namespace: <target managed cluster>
  name: hello-work-demo
spec:
  workload:
    manifests:
      - apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: hello
          namespace: default
        spec:
          selector:
            matchLabels:
              app: hello
          template:
            metadata:
              labels:
                app: hello
            spec:
              containers:
                - name: hello
                  image: quay.io/asmacdo/busybox
                  command:
                    ["sh", "-c", 'echo "Hello, Kubernetes!" && sleep 3600']
```

### Status tracking
Work agent会追踪`ManifestWork`中定义的所有资源并更新其状态。`ManifestWork`中有两种状态:
- `resourceStatus` 跟踪`ManifestWork`中每个清单的状态
- `conditions` 反映了`ManifestWork`的整体状态。 

```
apiVersion: work.open-cluster-management.io/v1
kind: ManifestWork
metadata: ... 
spec: ...
status:
  conditions:
    - lastTransitionTime: "2021-06-15T02:26:02Z"
      message: Apply manifest work complete
      reason: AppliedManifestWorkComplete
      status: "True"
      type: Applied
    - lastTransitionTime: "2021-06-15T02:26:02Z"
      message: All resources are available
      reason: ResourcesAvailable
      status: "True"
      type: Available
  resourceStatus:
    manifests:
      - conditions:
          - lastTransitionTime: "2021-06-15T02:26:02Z"
            message: Apply manifest complete
            reason: AppliedManifestComplete
            status: "True"
            type: Applied
          - lastTransitionTime: "2021-06-15T02:26:02Z"
            message: Resource is available
            reason: ResourceAvailable
            status: "True"
            type: Available
        resourceMeta:
          group: apps
          kind: Deployment
          name: hello
          namespace: default
          ordinal: 0
          resource: deployments
          version: v1
```
#### Fine-grained field values tracking
可以通过设置`ManifestWork`中的`FeedbackRule`，让work agent只聚合已分发资源中的某些字段并将其报告给中心集群：
```
apiVersion: work.open-cluster-management.io/v1
kind: ManifestWork
metadata: ...
spec:
  workload: ...
  manifestConfigs:
    - resourceIdentifier:
        group: apps
        resource: deployments
        namespace: default
        name: hello
      feedbackRules:
        - type: WellKnownStatus
        - type: JSONPaths
          jsonPaths:
            - name: isAvailable
              path: '.status.conditions[?(@.type=="Available")].status'
```

feedback rules规定work agent定期获取资源的最新状态，并仅从其中抓取那些期望的字段，这有助于减少状态信息的负载大小。除非最新值发生更改或者不同于先前记录的值，`ManifestWork`上收集的反馈值不会更新。目前支持两种FeedbackRule：

- WellKnownStatus：为那些知名的 kubernetes 资源使用预先构建的反馈值模板。
- JSONPaths：从资源中选择有效 Kubernetes JSON 路径。当前支持的类型是整数、字符串和布尔值。

默认的反馈值抓取间隔为 30 秒，可以通过在work agent上设置`--status-sync-interval`来覆盖它。时间太短会给托管集群的控制平面造成过大的负担，因此一般推荐的时间间隔下限为 5 秒。

最后，从feedback rules中抓取的值将显示在状态中：
```
apiVersion: work.open-cluster-management.io/v1
kind: ManifestWork
metadata: ...
spec: ...
status:  
  resourceStatus:
    manifests:
    - conditions: ...
      resourceMeta: ...
      statusFeedback:
        values:
        - fieldValue:
            integer: 1
            type: Integer
          name: ReadyReplicas
        - fieldValue:
            integer: 1
            type: Integer
          name: Replicas
        - fieldValue:
            integer: 1
            type: Integer
          name: AvailableReplicas
        - fieldValue:
            string: "True"
            type: String
          name: isAvailable
```

### Garbage collection

为了确保`ManifestWork`应用的资源被可靠地记录，work agent在托管集群上为每个`ManifestWork`创建一个`AppliedManifestWork`，作为与`ManifestWork`相关的资源的锚点。当`ManifestWork`被删除时，`ManifestWork`将保持在`deleting`状态, 工作代理运行`Foreground deletion`直到其所有相关资源在托管集群中被完全清理。

#### Delete options

删除`ManifestWork`时，用户可以明确选择不回收应用的资源。这时应在`ManifestWork`中指定`deleteOption`。默认情况下, `deleteOption`设置为 Foreground，这意味着受管集群上的资源将随着`ManifestWork`的移除而被删除。也可以将其设置为`Orphan`，这样申请的资源就不会被删除。这是一个例子：
```
apiVersion: work.open-cluster-management.io/v1
kind: ManifestWork
metadata: ...
spec:
  workload: ...
  deleteOption:
    propagationPolicy: Orphan
```


或者，也可以通过将`deleteOption`设置为`SelectivelyOrphan`来指定`ManifestWork`中定义的某个资源是`Orphan`。这是一个指定了 `SelectivelyOrphan`的示例。它确保在保留`Service`的同时删除 ManifestWork 中指定的`Deployment`资源。
```
apiVersion: work.open-cluster-management.io/v1
kind: ManifestWork
metadata:
  name: selective-delete-work
spec:
  workload: ...
  deleteOption:
    propagationPolicy: SelectivelyOrphan
    selectivelyOrphans:
      orphaningRules:
      - group: ""
        resource: services
        namespace: default
        name: helloworld
```

### Resource Race and Adoption

可以为同一集群的相同资源创建两个`ManifestWorks`。例如，用户可以在`cluster1`上创建两个`Manifestworks`，并且两个`Manifestworks`在`default`命名空间中都有名为`hello`的deplyment。如果该deployment的具体内容不同，则两个`ManifestWork`会发生冲突，因为每个 `ManifestWork`都是平等的，每个`ManifestWork`都在声明资源的所有权。

当其中一个`ManifestWork`被删除时，无论是否设置了`DeleteOption`，都不会删除应用的资源，因为剩余的`ManifestWork`仍将保留资源的所有权。

## ManagedClusterSet
`ManagedClusterSet`是`Hub Cluster`中的cluster-scoped API，它可以用于将一些托管集群分组到一个集合中，以便`Hub Cluster`管理员可以在更高级别上操作这些集群。这个概念来自 Kubernetes `SIG-Multicluster` 的增强。集合中的成员集群应该具有共同/相似的属性，例如使用的目标和部署的region。每个 `ManagedClusterSet`可以由不同的`Hub Cluster`管理员管理，也可以通过将不同的Cluster Set绑定到`Hub Cluster`中的`workspace namespace`来隔离它们的 RBAC 权限。Cluster Set管理员在可以使用 Placement API 等灵活地操作`workspace namespace`中的成员集群。

下图显示了Cluster Set如何工作的层次结构：
![clusterset-explain]((../../../assets/images/posts/clusterset-explain.svg)

`ManagedClusterSet`和`workspace namespace`具有 M*N 关系：
- 将多个Cluster Set绑定到一个工作空间命名空间，表示该命名空间的管理员可以操作上这些Cluster Sets中的成员集群。
- 将一个Cluster Set绑定到多个工作空间命名空间，表示Cluster Set可以同时从所有绑定的命名空间中进行操作。

### Operates ManagedClusterSet using clusteradm
#### Creating a ManagedClusterSet
使用下面的Command可以创建一个Cluster Set:
```
$ clusteradm create clusterset example-clusterset
$ clusteradm get clustersets
NAME                BOUND NAMESPACES    STATUS
example-clusterset                      No ManagedCluster selected
```

新创建的Cluster Set是空的，我们可以在后面添加Member Cluster。
#### Adding a ManagedCluster to a ManagedClusterSet
运行下面的命令添加Cluster 到 ClusterSet
```
$ clusteradm clusterset set example-clusterset --clusters managed1
$ clusteradm get clustersets
NAME                BOUND NAMESPACES    STATUS
example-clusterset                      1 ManagedCluster selected
```

将集群添加到集群集将要求管理员在`Hub Cluster`中具有`managedclustersets/join`访问权限。
```
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata: ...
rules:
  - apiGroups:
      - cluster.open-cluster-management.io
    resources:
      - managedclusters
    verbs:
      - update
  - apiGroups:
      - cluster.open-cluster-management.io
    resources:
      - managedclustersets/join
    verbs:
      - create
```

现在Cluster Set中包含 1 个有效集群，为了操作该Cluster Set，我们应该将其绑定到现有命名空间以使其成为`workspace namespace`。
#### Bind the clusterset to a workspace namespace

运行以下命令将Cluster Set绑定到命名空间。请注意，命名空间`不能是`现有的`cluster namespace`（即与托管集群具有相同名称的命名空间）。将Cluster Set绑定到命名空间意味着授予从该命名空间对其成员集群的访问权限。绑定过程需要在`Hub Cluster`中具有`managedclustersets/bind`权限。

```
$ clusteradm clusterset bind example-clusterset --namespace default
$ clusteradm get clustersets
NAME                BOUND NAMESPACES    STATUS
example-clusterset  default             1 ManagedCluster selected
```

```
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata: ...
rules:
  - apiGroups:
      - cluster.open-cluster-management.io
    resources:
      - managedclustersets/bind
    verbs:
      - create
```
### A glance at the “ManagedClusterSet” API
```
kubectl describe managedclusterset

apiVersion: cluster.open-cluster-management.io/v1beta1
kind: ManagedClusterSet
metadata:
  name: example-clusterset
spec: {}
status:
  conditions:
  - lastTransitionTime: "2022-02-21T09:24:38Z"
    message: 1 ManagedClusters selected
    reason: ClustersSelected
    status: "False"
    type: ClusterSetEmpty
```
### Default ManagedClusterSet
为了便于管理，我们引入了一个名为`default`的`ManagedClusterSet`。最初将自动创建一个默认的`ManagedClusterSet`。任何未指定`ManagedClusterSet`的集群都将添加到该Cluster Set中。用户可以使用以下命令将集群从默认Cluster Set移动到另一个Cluster Set：
```
clusteradm clusterset set target-clusterset --clusters cluster-name
```

## Placement
`placement`用于在一个或多个`ManagedClusterSet`中动态地选择一组托管集群，以便更高级别的用户可以将Kubernetes资源复制到成员集群或运行高级工作负载，例如多集群调度。

调度过程的“输入”和“输出”被解耦为两个独立的 Kubernetes API:
- Placement
- PlacementDecision
  
如下图所示，在 Placement API 的Spec中规定了调度策略，`Hub Cluster`中的Placement 控制器将帮助我们从给定的Cluster Set中动态选择一组托管集群。

PlacementDecision API 中的调度结果设计为以页面索引作为名称后缀进行分页，以避免底层 Kubernetes API 框架的`too large object`问题。

![placement-explain]((../../../assets/images/posts/placement-explain.svg)

遵循 Kubernetes 原有调度框架的架构，多集群调度在内部逻辑上分为两个阶段：

- Predicate：所选集群需达到的硬性要求。
- Prioritize：按软要求对集群进行排序，并从中选择一个子集。

### Predicates
#### Label/Claim selection
在Predicates部分，可以按标签或`clusterClaims`选择集群。例如，可以选择 3 个带有标签`purpose=test`和`clusterClaim` `platform.open-cluster-management.io=aws`的集群，如以下示例所示：

```
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: placement1
  namespace: default
spec:
  numberOfClusters: 3
  clusterSets:
    - prod
  predicates:
    - requiredClusterSelector:
        labelSelector:
          matchLabels:
            purpose: test
        claimSelector:
          matchExpressions:
            - key: platform.open-cluster-management.io
              operator: In
              values:
                - aws
```
#### Taints/Tolerations
为了支持过滤 unhealthy/not-reporting 的集群并防止工作负载被放置在不健康或无法访问的集群中，引入了类似在 Kubernetes 中的 taint/toleration 概念。它允许用户添加自定义污点以从Placement中取消选择集群。当用户想要将集群设置为维护模式并从该集群中逐出工作负载时，这很有用。

在 OCM 中，`Taints`和`Tolerations`协同工作，让用户可以更灵活地控制托管集群的选择。污点是`ManagedCluster`的属性，它们允许`Placement`在Predicate阶段排斥一组 ManagedCluster。

`Tolerations`应用于 Placements，允许 Placements 选择具有匹配污点的`ManagedCluster`。在`tolerations`部分，包括以下字段：

- Key (optional): tolerations键。
- Value (optional): tolerations值。
- Operator (optional): 运算符表示键与值的关系。有效的运算符是`Exists`和`Equal`。
- Effect (optional): Effect 指示要匹配的污点效果。空意味着匹配所有污点效果。
- TolerationSeconds (optional): TolerationSeconds 表示容忍（必须是 NoSelect/PreferNoSelect，否则该字段被忽略）污点的时长。默认值为`nil`，表示它永远容忍污点。计算`TolerationSeconds`的开始时间是`Taint`中的`TimeAdded`，而不是集群调度时间或`TolerationSeconds`添加时间。

以下示例显示了如何容忍带有污点的集群，假设有一个带有taints的集群：
```
apiVersion: cluster.open-cluster-management.io/v1
kind: ManagedCluster
metadata:
  name: cluster1
spec:
  hubAcceptsClient: true
  taints:
    - effect: NoSelect
      key: gpu
      value: "true"
      timeAdded: '2022-02-21T08:11:06Z'
```

默认情况下，placement不会选择该集群。除非Placement具有如下的tolerations：
```
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: placement1
  namespace: default
spec:
  tolerations:
    - key: gpu
      value: "true"
      operator: Equal
```

### Prioritizers
#### Score-based prioritizer

在`priorityrPolicy`部分，可以定义优先级的策略。例如，可以选择 2 个具有最大可用内存和最大addon score cpuratio的集群，并固定放置决策，如以下示例所示。
```
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: placement1
  namespace: default
spec:
  numberOfClusters: 2
  prioritizerPolicy:
    mode: Exact
    configurations:
      - scoreCoordinate:
          builtIn: ResourceAllocatableMemory
      - scoreCoordinate:
          builtIn: Steady
        weight: 3
      - scoreCoordinate:
          type: AddOn
          addOn:
            resourceName: default
            scoreName: cpuratio
```
- mode的可选值有：`Exact`、`Additive`、`""`，其中 `""` 默认为`Additive`。
  - 在`Additive`模式下，任何未明确枚举的优先级都使用其默认的配置启用，其中`Steady`和`Balance`优先级的权重为`1`，而其他优先级的权重为`0`。`Additive`不需要配置所有优先级。默认配置将来可能会更改，并且会添加其他优先级。
  - 在`Exact`模式下，任何未明确枚举的优先级都被加权为零。 `Exact`需要知道您想要的全部优先级，但要避免版本之间的行为变化。
- `configurations`代表优先级的配置。
  - `scoreCoordinate` 表示优先级和score source的配置。
    - `type`定义优先级分数的类型。类型为`BuiltIn`、`AddOn`或`“”`，其中`“”`默认为`BuiltIn`。当类型为`BuiltIn`时，必须指定`BuiltIn`优先级名称。当类型为`AddOn`时，需要在`AddOn`中配置score source。
    - `builtIn`定义了`BuiltIn`优先级的名称。以下是有效的内置优先级名称。
      - `Balance`：平衡集群之间的决策。
      - `Steady`：确保现有决策稳定。
      - `ResourceAllocatableCPU & ResourceAllocatableMemory`：根据allocatable对集群进行排序。
    - `addOn`定义资源名称和分数名称。引入`AddOnPlacementScore`来描述插件分数。
      - `resourceName` 定义`AddOnPlacementScore`的资源名称。placement prioritizer选择按此名称的`AddOnPlacementScore` CR。
      - `scoreName` 定义 `AddOnPlacementScore` 中的分数名称。 `AddOnPlacementScore` 包含一个分数名称和分数值的列表，`ScoreName`指定了优先级要使用的分数。
  - `weight`定义了优先级的权重。该值必须在 [-10,10] 范围内。每个优先级将为cluster计算一个在 [-100, 100] 范围内的整数分数。一个Cluster的最终分数将是 sum(weight * priorityr_score)。权重越高表示优先级在集群选择中的权重越大，而权重为 0 表示优先级被禁用。负权重表示想要选择最后一个。


一系列`PlacementDecision`将由placement控制器在同一个命名空间中创建，每个都带有`cluster.open-cluster-management.io/placement={placement name}`的标签。 `PlacementDecision`包含集群选择的结果，如以下示例所示。
```
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: PlacementDecision
metadata:
  labels:
    cluster.open-cluster-management.io/placement: placement1
  name: placement1-decision-1
  namespace: default
spec:
  decisions:
    - clusterName: cluster1
    - clusterName: cluster2
    - clusterName: cluster3
```
`PlacementDecision`可由其他operand使用，以决定如何将工作负载放置在多个集群中。

### Extensible scheduling

在基于placement resource的调度中，在某些情况下，优先级需要额外的数据（不仅仅是ManagedCluster提供的默认值）来计算托管集群的分数。例如，根据从监控系统获取的集群的 cpu 或内存使用数据来调度集群。

一个新的 API `AddOnPlacementScore`可以用来来支持基于自定义分数的可扩展的调度方式。
- 作为用户，如上一节所述，可以在placement yaml 中指定分数来选择集群。
- 作为分数提供者，第 3 方控制器可以在`Hub Cluster`器或托管集群上运行，以维护`AddOnPlacementScore`的生命周期并将分数更新到其中。
请参阅增强功能以​​了解更多信息。
## Add-ons

Open-cluster-management 有一个名为`addon-framework`的内置机制，可帮助开发人员开发基于基础组件的扩展，以便在一些特殊案例中使用多个集群。一个典型的插件应该由两种组件组成：

- Addon Agent：托管集群中的一个 kubernetes 控制器，它为`Hub Cluster`管理员管理托管集群。一个典型的插件代理将通过从`Hub Cluster`订阅prescriptions（以 CustomResources 的形式）来工作，然后像普通的 kubernetes Operator一样协调托管集群的状态。
- Addon Manager：`Hub Cluster`中的一个 kubernetes 控制器，它通过 ManifestWork api 将 manifests apply到托管集群。除了资源分发之外，管理器还可以选择管理插件代理的`CSR`的生命周期，甚至管理与`CSR`请求身份绑定的`RBAC`权限。

一般来说，如果在托管集群内工作的管理工具需要区分每个托管集群的配置，则将其实现建模为Addon Agent会很有帮助。每个Agent的配置都应该保存在`Hub Cluster`中，因此`Hub Cluster`管理员将能够指定代理以声明的方式完成其工作。抽象地说，通过插件，我们将多集群控制平面解耦为：
- 策略调度
- 执行

Addon Manager实际上并不直接将任何更改应用到托管集群，而是将其prescriptions放置到为托管集群分配的专用命名空间中。然后Addon Agent提取prescriptions并执行。

除了在Agent处理之前分发配置，Addon Manager还会在Agent启动之前自动做一些繁琐的准备，例如：
- CSR的申请、批准和签署。
- 注入和管理用于访问`Hub Cluster`的客户端凭据。
- `Hub Cluster`或托管集群中的Agent的 RBAC 权限。
- 安装策略。

### Architecture

以下架构图显示了插件管理器和插件代理之间的协调是如何工作的。
![addon-architecture]((../../../assets/images/posts/addon-architecture.svg)

从用户的角度来看，要将插件安装到`Hub Cluster`，`Hub Cluster`管理员应该将全局唯一的`ClusterManagementAddon`资源注册在`Hub Cluster`中。例如，helloworld 插件可以通过创建以下内容注册到`Hub Cluster`：

```
apiVersion: addon.open-cluster-management.io/v1alpha1
kind: ClusterManagementAddOn
metadata:
  name: helloworld
spec:
  addOnMeta:
    displayName: helloworld
```

在`Hub Cluster`上运行的插件管理器负责为每个托管集群配置插件代理的安装。当用户想要为某个托管集群启用插件时，用户应该在集群命名空间上创建一个 `ManagedClusterAddon`资源。 `ManagedClusterAddon`的名称应该与对应的`ClusterManagementAddon`的名称相同。例如，以下示例在`cluster1`中启用 helloworld 插件：
```
apiVersion: addon.open-cluster-management.io/v1alpha1
kind: ManagedClusterAddon
metadata:
  name: helloworld
  namespace: cluster1
spec:
  installNamespace: helloworld
```

通过简单地从`Hub Cluster`中删除相应的`ClusterManagementAddon`资源，也可以轻松卸载插件。卸载后，OCM 平台将自动删除`Hub Cluster`或管理集群中的所有addon相关的组件。

当通过`kubectl`列出插件时，插件实例的健康状况是可见的：
```
$ kubectl get managedclusteraddon -A
NAMESPACE   NAME                     AVAILABLE   DEGRADED   PROGRESSING
<cluster>   <addon>                  True        
```
## Policy
policy framework具有以下 API 概念：
- `Policy Templates` 是执行所需检查或操作的策略。例如，`ConfigurationPolicy`对象嵌入在Policy 对象中的`policy-templates`数组下。这些不能单独部署到托管集群。
- `Policy`是`Policy Templates`的一种分组机制，是`Hub Cluster`上最小的可部署单元。嵌入式`Policy Templates`被分发到适用的托管集群，并由适当的策略控制器执行。
- `PolicySet` 是 `Policy` 对象的分组机制。`PolicySet` 是一个可部署的单元，其分布由 `Placement` 控制。
- `PlacementBinding` 将 `Placement` 绑定到 `Policy` 或 `PolicySet`。

嵌入`ConfigurationPolicy`策略模板以管理名为“prod”的命名空间的简单Policy示例。

```
apiVersion: policy.open-cluster-management.io/v1
kind: Policy
metadata:
  name: policy-namespace
  namespace: policies
  annotations:
    policy.open-cluster-management.io/standards: NIST SP 800-53
    policy.open-cluster-management.io/categories: CM Configuration Management
    policy.open-cluster-management.io/controls: CM-2 Baseline Configuration
spec:
  remediationAction: enforce
  disabled: false
  policy-templates:
    - objectDefinition:
        apiVersion: policy.open-cluster-management.io/v1
        kind: ConfigurationPolicy
        metadata:
          name: policy-namespace-example
        spec:
          remediationAction: inform
          severity: low
          object-templates:
            - complianceType: MustHave
              objectDefinition:
                kind: Namespace # must have namespace 'prod'
                apiVersion: v1
                metadata:
                  name: prod
```
首先，注意到annotations。这些是用于提供信息的标准注释，可供用户界面、自定义报告脚本或与 OCM 集成的组件使用。

接下来是可选的`spec.remediationAction`字段。这决定了策略控制器在发现违规时是否应该`inform`或者`enforce`，并覆盖每个策略模板上的 `remediationAction`字段。当设置为`inform`时，如果策略模板检测到未满足所需状态，则该策略将变为不合规。设置为强制执行时，策略控制器会在必要且可行时执行所需状态。

`policy-templates`数组包含一个名为`policy-namespace-example`的`ConfigurationPolicy`。此`ConfigurationPolicy`将`remediationAction`设置为`inform`，但它会被可选的全局`spec.remediationAction`覆盖。`severity`同注释一样只是处于类似的信息目的。

最有趣的部分是嵌入式`ConfigurationPolicy`下的对象模板部分。此例子描述了该策略适用于 prod 命名空间对象。 `ConfigurationPolicy` 将采取的操作由`complianceType`确定。在此例中，它被设置为`MustHave`，这意味着如果 prod 命名空间对象不存在，它将会被创建。其他`complianceType`包括`MustNotHave`和`MustOnlyHave`。 `MustNotHave`将删除 prod 命名空间对象。 `MustOnlyHave`将确保 prod 命名空间对象仅与`ConfigurationPolicy`中定义的一样。

当`Policy`绑定到`Placement`时，`Policy`状态将显示在与绑定`Placement`匹配的每个集群：
```
status:
  compliant: Compliant
  placement:
    - placement: placement-hub-cluster
      placementBinding: binding-policy-namespace
  status:
    - clustername: local-cluster
      clusternamespace: local-cluster
      compliant: Compliant
```

### PlacementBinding
`PlacementBinding`将`Placement`绑定到`Policy`或`PolicySet`。

下面是一个`PlacementBinding`示例，它将`policy-namespace Policy`绑定到`placement-hub-cluster Placement`。
```
apiVersion: policy.open-cluster-management.io/v1
kind: PlacementBinding
metadata:
  name: binding-policy-namespace
  namespace: policies
placementRef:
  apiGroup: cluster.open-cluster-management.io
  kind: Placement
  name: placement-hub-cluster
subjects:
  - apiGroup: policy.open-cluster-management.io
    kind: Policy
    name: policy-namespace
```
绑定策略后，它将被分发到与placement匹配的托管集群并由其执行。

### PolicySet

`PolicySet`是`Policy`对象的分组机制。`PolicySet`中汇总了所有分组的策略对象的合规性。`PolicySet`是一个可部署的单元，当通过 `PlacementBinding`绑定时，它的分发由`Placement`控制。

这会启用了一个工作流，其中由Subject专家编写Pollicy对象，然后 IT 管理员创建一个`PolicySet`，将先前编写的Pollicy对象分组并将`PolicySet`绑定到Placement并部署`PolicySet`。

`PolicySet`的示例如下所示。
```
apiVersion: policy.open-cluster-management.io/v1beta1
kind: PolicySet
metadata:
  name: acm-hardening
  namespace: policies
spec:
  description:
    Apply standard best practices for hardening your Open Cluster Management installation.
  policies:
    - policy-check-backups
    - policy-managedclusteraddon-available
    - policy-subscriptions
```

### Policy Templates

configuration policies支持在对象定义中包含 Golang 文本模板。这些模板在运行时使用与该集群相关的配置在`Hub Cluster`或目标托管集群上解析。这能够定义具有动态内容的configuration policies，并`inform`或`enforce`目标集群定制的 Kubernetes 资源。

模板语法必须遵循 Golang 模板语言规范，并且从解析的模板生成的资源定义必须是有效的 YAML。 模板验证中的任何错误都会显示为policy violations。当您使用自定义模板函数时，这些值会在运行时被替换。