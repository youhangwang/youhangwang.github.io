---
title: ODF Disaster Recovery
tags: ODF DR
---

Disaster Recovery最基本的要求是从灾难中恢复关键应用程序。一旦灾难发生，可能会丢失的大量的数据并且对商业信誉造成极大的损失。目前 Kubernetes 平台的多集群解决方案已经慢慢成熟，基于该功能，可以提供应用程序和集群Failover服务。


## Disaster Recovery goals & objectives

Disaster Recovery是从自然或人为灾难中恢复和继续关键业务的能力。它是任何主要组织的整体业务连续性战略的组成部分，旨在重大不利事件期间保持业务运营的连续性。Recovery goals通常表示为恢复点目标 (RPO) 和恢复时间目标 (RTO)：
- RPO 是衡量对持久性数据进行备份或快照的频率的度量。实际上，RPO 表示中断后将丢失或需要重新输入的数据量。对于同步存储复制，数据丢失量将为零。对于异步存储复制，数据丢失量与复制间隔相关（例如，最大数据丢失 5 分钟）。
- RTO 是应用程序或服务可以容忍的停机时间。 RTO 回答了以下问题：“在我们收到业务中断通知后，我们的系统需要多长时间才能恢复？”。

## Solution Components

- Red Hat Advanced Cluster Management for Kubernetes
- Red Hat OpenShift Data Foundation (ODF)
- Red Hat Ceph Storage (RHCS)
- OpenShift DR (ODR)
  - OpenShift DR Hub Operator
  - OpenShift DR Cluster Operator
  - OpenShift Data Foundation Multicluster Orchestrator

## Application/Workload  specific considerations

- 支持将 PVC 资源作为其 git repo 一部分的应用程序
  - 这使 ACM 能够确保 PVC 被删除，从而使 ODR 能够安全地将image角色从primary切换到secondary。
- 如果多个应用程序部署到同一个命名空间，它们的 PVC 应该被唯一标记，以便能够对属于托管集群上的应用程序的 PVC 进行分组。
  - 还需要将唯一标签提供给 ODR DRPlacementControl 资源。
- 所有需要 DR 保护的应用程序 PVC 都应使用由 ODF csi 驱动程序提供的 StorageClass，在 RDR 的情况下，仅支持 RBD/Block SC。
- 必须使用 ACM 中心应用程序管理工作流程部署应用程序。

## Key Operations

以下大部分关键的 DR 操作由 ODR Operator自动执行：

- 配置 Ceph-RBD pool Mirroring（用于 Regional-DR）
  - 包括启动RBD mirror service，peering 2 个cluster，确保跨集群网络的配置（submariner）
- 为 DR Ramen operator配置每个集群的 S3 存储，从而存储/恢复 PV 的元数据
- 为应用程序的所有 PVC 启用mirroring并管理mirror角色：Primary还是secondary，具体取决于工作负载在peer cluster中的放置位置和方式
- 在灾难发生和恢复的情况下协调应用程序placement

DR所使用Hub上的资源包括：
- MirrorPeer (Hub)（由用户/UI创建）
  - 负责设置 RBD mirroring和每个 PVC 的 ODF 功能以实现跨对等集群的mirroring
  - 负责为 Ramen（DR Operator）创建所需的 S3 存储桶以供使用
- DRCluster (Hub)（由 MirrorPeer 创建）
  - 将 ManagedCluster 定义为 DR peer，并且还包含此集群所需的 S3 存储配置
- DRPolicy（集线器）（由用户/UI创建）
  - 定义 2 个 DRCluster 之间的同步/异步策略，以及所需的异步计划
- DRPlacementControl (Hub)（由用户/UI创建）
  - 每个 application 的 DR placement orchestrator，根据 DRPolicy 放置应用程序
  - 通过适当地调度 ACM PlacementRule 实现应用程序移动编排
  - 负责指挥peer cluster上的 PVC protection 和 state 

DR所使用Managed cluster上的资源包括：
- VolumeReplicationGroup (ManagedCluster)（由 DRPlacementControl 创建）
  - 根据标签选择器定义集群上哪些 PVC 需要 DR 保护
  - 还可以进一步定义：
    - PVC 的期望状态（主要/次要）
    - 保护模式（异步/同步）
    - 异步时的复制间隔
    - 用于 PV 资源的 S3 配置文件（用于存储/恢​​复）
- VolumeReplicationClass (ManagedCluster)（由 MirrorPeer 创建）
  - 像 StorageClass 一样，定义一个类，其中包含用于protection的复制间隔，以及哪个 CSI 驱动程序作用于指向该类的 VolumeReplication 资源
- VolumeReplication (ManagedCluster)（由 VolumeReplicationGroup 创建）
  - 表示每个 PVC 的复制需求，在创建时启用并设置为主要/次要角色
  - 调用 Ceph-CSI 驱动程序接口以发出所需的操作达到所需的状态

## Operators in DR
![openshift-dr-operators.png](../../../assets/images/posts/openshift-dr-operators.png)

- odf-multicluster-console（Hub）
  - 提供UI
  - 根据用户操作创建 MirrorPeer 和 DRPolicy cluster-scoped的资源
- odfmo-controller-manager（Hub）
  - 协调 MirrorPeer 资源
  - 在Hub Cluster上创建 DRCluster 资源
  - 在 ManagedCluster 上创建所需的 VolumeReplicationClass Cluster-scoped资源
- ramen-dr-hub-operator（Hub）
  - 协调 DRPlacementControl、DRPolicy、DRCluster 资源
  - 在所需的 ManagedCluster 上创建 VolumeReplicationGroup Namespaced-scoped资源
- token-exchange-agent (ManagedCluster)
  - 创建 Nooba 支持的 S3 ObjectBucketClaim 供 DR 解决方案使用
  - 通过 Hub 促进各种 Secret 交换到对等的 ManagedClusters
- ramen-dr-cluster-operator（ManagedCluster）
  - 协调所有命名空间中的 VolumeReplicationGroup 资源
  - 为每个需要 DR 保护的 PVC 创建一个 VolumeReplication 资源
  - 利用 S3 存储桶为每个受保护的 PVC 存储所需的 PV 元数据
- volume-replication (sidecar container in csi-rbdplugin-provisioner) (ManagedCluster)
  - 协调所有命名空间中的 VolumeReplication 资源
- rook-ceph-rbd-mirror-a (ManagedCluster)
  - Ceph RBD mirror daemon负责将每个image的快照mirror到对等的 Ceph 集群

## Resources managed by Openshift DR

![openshfit-dr-managed-resource.png](../../../assets/images/posts/openshfit-dr-managed-resource.png)


## Regional-DR for OpenShift Container Platform (OCP) applications

该能力提供跨地理分散站点的持久卷数据和元数据复制。在公有云中，这些类似于防止Region failure。 Regional-DR 确保在地理区域不可用期间的业务连续性，接受一些可预测数量的数据丢失。在几分钟内实现 RPO 和 RTO 的恢复目标而不是几小时是可能的（例如，5 分钟）。

- RPO: Mins
- RTO: Mins
- 异步卷复制: 低 RPO
  - OpenShift Data Foundation (ODF) 支持跨集群复制数据卷，复制间隔低至 1 分钟
  - ODF Storage operator 为 PV 同步卷持久数据和 kubernetes 元数据
  - 对等集群之间没有距离限制
- 自动故障转移管理: 低 RTO
  - ACM 和 ODF DR Operator可在应用程序粒度上实现故障转移和故障回复自动化
  - 两个集群都保持活动状态，应用程序由备用集群分发和保护

![odf-regionaldr.png](../../../assets/images/posts/odf-regionaldr.png)

### [Regional-DR Config](./2022-10-15-odf-regionaldr.md)

### RDB Mirroring

RBD Mirroring是在多个 Ceph 集群之间异步复制 RBD image。此功能在两种模式下可用：

- 基于日志：每次对 RBD image的写入都会在修改实际映像之前记录到关联的日志中。远程集群将从这个关联的日志中读取并将更新re-play到其本地image。
- 基于快照：此模式使用定期计划或手动创建的 RBD image快照在集群之间复制crash-consistent的 RBD image。

1. Create RBD Pools

在每个peer cluster上执行以下步骤以创建并启用镜像的pool：

- 通过在 CephBlockPool CR 中添加 spec.mirroring 部分来创建启用镜像的 RBD 池：

```
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: mirroredpool
  namespace: rook-ceph
spec:
  replicated:
    size: 1
  mirroring:
    enabled: true
    mode: image
```

```
kubectl create -f pool-mirrored.yaml
```

- 在peer cluster上重复该步骤。

2. Bootstrap Peers

为了让`rbd-mirror` daemon发现它的peer cluster，必须注册peer cluster并且必须创建一个user accoun。

以下步骤能够引导对等点相互发现和验证：

- 对于引导peer cluster，需要其bootstrap secret。要确定包含引导程序secret的名称，在远程集群 (cluster-2) 上执行以下命令
```
[cluster-2]$ kubectl get cephblockpool.ceph.rook.io/mirroredpool -n rook-ceph -ojsonpath='{.status.info.rbdMirrorBootstrapPeerSecretName}'
```

在这里，`pool-peer-token-mirroredpool` 是所需的引导程序的secret名称。

- secret `pool-peer-token-mirroredpool` 包含与令牌相关的所有信息，需要注入到peer cluster，以获取解码后的secret：
```
[cluster-2]$ kubectl get secret -n rook-ceph pool-peer-token-mirroredpool -o jsonpath='{.data.token}'|base64 -d
eyJmc2lkIjoiNGQ1YmNiNDAtNDY3YS00OWVkLThjMGEtOWVhOGJkNDY2OTE3IiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFDZ3hmZGdxN013R0JBQWZzcUtCaGpZVjJUZDRxVzJYQm5kemc9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMzkuMzY6MzMwMCx2MToxOTIuMTY4LjM5LjM2OjY3ODldIn0=
```

- 使用此解码值，在主站点 (cluster-1) 上创建一个secret：
```
[cluster-1]$ kubectl -n rook-ceph create secret generic rbd-primary-site-secret --from-literal=token=eyJmc2lkIjoiNGQ1YmNiNDAtNDY3YS00OWVkLThjMGEtOWVhOGJkNDY2OTE3IiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFDZ3hmZGdxN013R0JBQWZzcUtCaGpZVjJUZDRxVzJYQm5kemc9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMzkuMzY6MzMwMCx2MToxOTIuMTY4LjM5LjM2OjY3ODldIn0= --from-literal=pool=mirroredpool
```

- 这样就完成了 cluster-1 与 cluster-2 对等的引导过程。
- 将cluster-2 代替 cluster-1之后重复该步骤 ，以完成跨两个peer cluster的引导过程。

3. Configure the RBDMirror Daemon

Replication由 rbd-mirror daemon 处理。 rbd-mirror daemon 负责从远程peer cluster中拉取image更新，并将它们应用于本地集群中的image。

rbd-mirror daemon的创建是通过自定义资源定义 (CRD) 完成的，如下所示：

- 创建 mirror.yaml，部署 rbd-mirror daemon

```
apiVersion: ceph.rook.io/v1
kind: CephRBDMirror
metadata:
  name: my-rbd-mirror
  namespace: openshift-storage
spec:
  # the number of rbd-mirror daemons to deploy
  count: 1
```
```
[cluster-1]$ kubectl create -f mirror.yaml -n rook-ceph
```
- 验证 rbd-mirror daemon pod 现在是否启动
```
[cluster-1]$ kubectl get pods -n rook-ceph
rook-ceph-rbd-mirror-a-6985b47c8c-dpv4k 1/1 运行 0 10s
```

- 验证rbd-mirror daemon运行状况是否正常
```
kubectl get cephblockpools.ceph.rook.io mirroredpool -n rook-ceph -o jsonpath='{.status.mirroringStatus.summary}'
{"daemon_health":"OK","health":"OK","image_health":"OK","states":{"replaying":1}}
```
在peer cluster上重复上述步骤。

4. Add mirroring peer information to RBD pools

每个 pool 都可以有自己的peer。要添加对等点信息，请修改已启用mirroring的pool更新 CephBlockPool CRD。

```
[cluster-1]$ kubectl -n rook-ceph patch cephblockpool mirroredpool --type merge -p '{"spec":{"mirroring":{"peers": {"secretNames": ["rbd-primary-site-秘密”]}}}}'
```

5. Create VolumeReplication CRDs

Volume Replication Operator 遵循控制器模式并为storage灾难恢复提供Extended API。Extended API 通过自定义资源定义 (CRD) 提供。在所有Peer Cluster上创建 VolumeReplication CRD。

```
kubectl create -f https://raw.githubusercontent.com/csi-addons/volume-replication-operator/v0.3.0/config/crd/bases/replication.storage.openshift.io_volumereplications.yaml
kubectl create -f https://raw.githubusercontent.com/csi-addons/volume-replication-operator/v0.3.0/config/crd/bases/replication.storage.openshift.io_volumereplicationclasses.yaml
```

6. Enable CSI Replication Sidecars


要实现 RBD Mirroring，需要在 RBD provisioner pod 中部署 `csi-omap-generator` 和 `volume-replication` 容器，默认情况下不启用。

- Omap generator：Omap generator是一个 sidecar 容器，当与 CSI provisioner pod 一起部署时，它会在 PV 和 RBD image之间生成internal CSI omap。这是必需的，因为静态 PV 在 DR 中跨peer cluster传输，因此需要保留 PVC 到storage映射。

- Volume Replication Operator：Volume Replication Operator 是一个 kubernetes operator，它为storage灾难恢复提供通用和可重用的 API。它基于 csi-addons/spec 规范，可供任何存储提供商使用。

在每个peer cluster上执行以下步骤以启用 OMap generator和 Volume Replication sidecars：

- 编辑 `rook-ceph-operator-config` configmap 并添加以下配置:

```
kubectl edit cm rook-ceph-operator-config -n rook-ceph
```

请添加以下属性：

```
data:
  CSI_ENABLE_OMAP_GENERATOR: "true"
  CSI_ENABLE_VOLUME_REPLICATION: "true"
```

- 使用这些设置更新 configmap 后，两个新的 sidecar 现在应该会在 CSI provisioner pod 中自动启动。
- 在peer cluster上重复这些步骤。



7. Volume Replication Custom Resources

VolumeReplication CRD 支持两种自定义资源：

- VolumeReplicationClass：VolumeReplicationClass 是一个集群范围的资源，包含与驱动程序相关的配置参数。它包含volume replication operator所需的storage admin信息。

- VolumeReplication：VolumeReplication 是一种命名空间资源，它包含对要复制的存储对象的引用以及与提供复制的驱动程序相对应的 VolumeReplicationClass。

8. Enable mirroring on a PVC


## Metro-DR for OpenShift Container Platform (OCP) applications

该能力提供跨地理分散站点的持久卷数据和元数据复制。在公有云中，这些类似于防止Avaliability Zone故障。 Metro-DR 确保数据中心不可用期间的业务连续性，不会丢失数据。实现 RPO 为零、无数据丢失和 RTO 在几分钟内（例如 5 分钟）的恢复目标是可能的。

- RPO: Mins
- RTO: Mins
- 同步卷复制: 零 RPO
  - 为部署在不同可用区的多个OCP集群提供完整的故障隔离配置
  - 外部 RHCS 存储集群提供跨多个 OCP 集群的持久卷同步复制，实现零 RPO
  - 站点之间的距离限制为 10ms RTT
- 自动故障转移管理: 低 RTO
  - ACM 和 ODF DR Operator可在应用程序粒度上实现故障转移和故障回复自动化
  - 需要第三个站点中的 Arbiter 节点用于存储集群监控服务以保持仲裁

虽然 Metro-DR 是无数据丢失 (RPO=0) 解决方案的主要选择。但仍然有这些限制：
- 仅限 ODF external mode： 需要熟悉 RHCS 存储管理
- 仅限本地部署 - 不支持公共云。
  - 需要第三个站点上的 Arbiter，可以在公共云上运行（<100 毫秒 RTT 延迟）
- MDR 的故障转移粒度是针对每个 OCP 集群的，这与 RDR 中提供的针对每个应用程序的粒度不同

Metro-DR 避免了跨站点部署一个 OCP 集群，stretch OCP 集群会导致集群不稳定并使day2的操作复杂化。

![odf-metrodr.png](../../../assets/images/posts/odf-metrodr.png)


### [Metro-DR Config](./2022-09-25-odf-metrodr.md)
